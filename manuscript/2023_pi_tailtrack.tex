
%%% PREAMBLE 
\documentclass[9pt,lineno]{RandlettLab_elife}
%\nolinenumbers
% Use the onehalfspacing option for 1.5 line spacing
% Use the doublespacing option for 2.0 line spacing
% Please note that these options may affect formatting.
% Additionally, the use of the \newcommand function should be limited.

\usepackage{lipsum} % Required to insert dummy text
\usepackage{listings}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{gensymb}
\DeclareSIUnit\Molar{M}
\usepackage{cancel}
% \usepackage[normalem]{ulem}
% \newcommand{\soutthick}[1]{%
%     \renewcommand{\ULthickness}{1.6pt}%
%        \sout{#1}%
%     \renewcommand{\ULthickness}{.4pt}% Resetting to ulem default
% }


\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ARTICLE SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\emph{pi\_tailtrack}: A compact, inexpensive, and open-source behaviour-tracking system for head-restrained zebrafish}


\author[1]{Owen Randlett}


\affil[1]{
Laboratoire MeLiS, UCBL - CNRS UMR5284 - Inserm U1314, Institut NeuroMyoGène, Faculté de Médecine et de Pharmacie, 8 avenue Rockefeller, 69008, Lyon, France 
}


\corr{owen.randlett@univ-lyon1.fr}{OR}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ARTICLE START
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle
\begin{abstract}

Quantifying animal behavior during microscopy is crucial to associate optically recorded neural activity with behavioural outputs and states. Here I describe an imaging and tracking system for head-restrained larval zebrafish compatible with functional microscopy. This system is based on the Raspberry Pi computer, Pi NoIR camera, and open-source software for the real-time tail segmentation and skeletonization of the zebrafish tail at over 100hz. This allows for precise and long-term analyses of swimming behaviour, that can be related to functional signals recorded in individual neurons. This system offers a simple but performant solution for quantifying the behavior of head-restrained larval zebrafish, which can be built for 340€.

\end{abstract}

\section{Introduction}

A chief application of the larval zebrafish for neuroscience is to image the activity of neurons in the intact and behaving animal using microscopy. This is facilitated by its translucent and small brain, measuring approximately 0.1 mm\textsuperscript{3}. By expressing genetically encoded indicators, such as the GCaMP Ca\textsuperscript{2+} sensors \citep{Akerboom2012, Chen2013}, signals related to the activity of practically any or all neurons can be recorded from the larval zebrafish brain \citep{Ahrens2012, Portugues2014}. 

Ca\textsuperscript{2+} imaging can be performed with standard microscopes, but such systems are not equipped for monitoring the behaviour of the animal. Therefore, any analyses directly relating neural activity to behaviour will require the integration of a behavioural recording apparatus. Behavioural recording is typically done in the context of custom-built microscopes, which can be designed explicitly with this behaviour-monitoring goal in mind. However, many groups (including my own) have neither the financial or technical means to implement such a complete system. We rely on microscope equipment in a shared core facility. Such microscopes generally cannot be substantially or permanently modified, and often present physical and optical constraints that make installing a behaviour imaging system challenging. 

Here I present a solution for this problem based on the Raspberry Pi computer, that I call \emph{pi\_tailtrack}. The system includes illumination, camera, computer and software, yielding a complete setup that is compact, inexpensive, and self-contained. The \emph{pi\_tailtrack} system can reliably track larval zebrafish behaviour at over 100hz while performing functional imaging experiments. 

\newpage
\section{Results and Discussion}

\subsection{Design goals}

I wanted to track the swimming behaviour of head-restrained larval zebrafish while performing Ca\textsuperscript{2+} imaging. There are many ways that this might be accomplished, but I wanted a system that was: 
\begin{enumerate}
    \item Able to identify and characterize individual swimming events while we are imaging the brain using 2-photon microscopy.
    \item Compact and self contained, so that it can be easily and rapidly installed and removed for our imaging sessions on a shared microscope.
    \item Made using low-cost and open source hardware and software to facilitate re-use in other contexts, and because I am a \xcancel{cheap} \emph{financially responsible} researcher.

\end{enumerate}

\subsection{Using a Raspberry Pi camera to image the larval zebrafish tail}

\begin{wrapfigure}{r}{.6\textwidth}
\includegraphics[width=\hsize]{Figure_Flowchart.png}
\caption{\textbf{Schematic for the \emph{pi\_tailtrack} apparatus.}
\\ The zebrafish larvae being imaged under the microscope is illuminated with infra-red (IR) LEDs, and imaged with the IR-sensitive Raspberry Pi NoIR Camera. Image acquisition and processing is done with a Raspberry Pi Computer and open-source Python packages. The zebrafish tail is identified and segmented in real-time as a sequence of 10 tail segments (green X's). 
}
\label{fig:1}
\end{wrapfigure}

\vspace{3mm}

The Raspberry Pi is a very inexpensive, credit-card-sized computer that plugs into a standard monitor, keyboard, and mouse. The Raspberry Pi's open-source nature and large user community, and its ability to control and interface with a variety of devices and sensors make it a powerful and accessible platform for developing and sharing custom neuroscience and behavioural research tools. Indeed many such systems have been developed in recent years based around the Raspberry Pi and the Pi Camera, and especially the IR-sensitive Pi NoIR camera, as an acquisition device \citep{Geissmann2017-gd, Maia_Chagas2017-mf, Saunders2019-ka, Tadres2020-as, Broussard2022-yd}. 

However, obtaining sufficient resolution and contrast to resolve the larval zebrafish tail is challenging since the tail is very narrow ($\approx$0.25mm diameter), and nearly transparent. This is especially true in de-pigmented animals that are generally used for brain imaging due to their lack of melanin pigment over the brain (e.g. \textit{mifta}/Nacre mutants, or larvae treated with N-Phenylthiourea). This also removes melanin pigment from the tail, increasing its transparency and making it harder to image and track. Thus, it was not clear if the 26€ Pi NoIR Camera would be up to this task. 

The stock lens configuration on the Pi Camera is also not designed for macro photography, and has a minimum focus distance of 50cm. But, extension tubes are a well-known macro-photography hack that work by increasing the distance between the lens and the camera \citep{enwiki:1118116052}. Increasing this distance acts to decreases the focus distance of the optical system, increasing the maximal magnification. By unscrewing the lens of the Pi NoIR camera until just before it falls off, it is possible to focus on objects at a 2 cm distance, allowing for sufficient magnification to observe the and track the tail of \textit{mifta} mutant zebrafish (\autoref{fig:1}, \autoref{fig:3}).

\begin{wrapfigure}{r}{.5\textwidth}
\includegraphics[width=\hsize]{Figure_MechDrawings.png}
\caption{\textbf{\emph{pi\_tailtrack} hardware.}
\\ \textbf{A)} CAD drawing of the main components of the apparatus. IR leds illuminate the zebrafish larvae that is head-restrained in agarose in a 35mm diameter petri dish lid. An IR filter blocks the visible stimulus lights (Red LEDs), and the microscope laser from reaching the Raspberry Pi NoIR camera suspended below the fish. ii) Wiring diagram for powering the IR LEDs. 
\\ \textbf{B)} same as (A), but with i) the 3D printed mount visible, and ii) the microscope objective and semicircle shape component to encircle the objective. 
\\ \textbf{C)} Photo of the apparatus within the microscope, without the semicircle shape component. IR LED light is visible as the indigo colour.   
} 

\label{fig:2}
\end{wrapfigure}

A second challenge is that larval zebrafish move their tails very rapidly, with a tail-beat frequency of between 20-40 hz for normal swimming, which can increase to 100 hz during burst/escape swimming \citep{Budick2000-bq, Muller2004-st, Severi2014-iw}. The V2.1 camera documentation indicates maximum frame rate of 30hz, which is insufficient for imaging tail dynamics. However, by adopting a cropped sensor configuration, and by omitting the JPG compression step in image processing, the camera can be pushed to image at up to 1000hz \citep{660hz}. I adopted a configuration where I image with a cropped sensor of 128x128 pixels, which gives sufficient spatial resolution to observe and track the tail of the fish, and most importantly, minimal CPU load. This frees the limited CPU resources on the Raspberry Pi to be used for real-time image processing and tail tracking. 

\subsection{Hardware Setup}

The short 2 cm focal distance between the animal and the camera allowed for a compact and direct imaging setup, where the camera is mounted directly below the larva (\autoref{fig:2}A). This avoids the need for any mirrors, and frees the space above the animal for the microscope objective, and any stimulus apparatus necessary. In our case we use red LEDs to provide visual stimuli to the larvae \citep{Lamire2023-di}. 

To illuminate the larvae and visualize the tail, I used 890nM IR LEDs. Using the IR LEDs as an oblique illumination source generated a nicely resolved image of the \textit{mifta} mutant zebrafish tail that was sufficient for reliable identification and tracking (\autoref{fig:3}). IR LEDs were wired in a simple circuit, with 10 LEDs in a series, powered by a 18V DC power supply and a 47ohm current limiting resistor (\autoref{fig:2}Ai). Using these exact Voltage/Resistance configurations is not important, provided a relevant power supply and resistor are chosen to match the LED characteristics (forward voltage =1.4V, current = 100mA, for our 890nM LEDs: see for example \href{https://www.amplifiedparts.com/tech-articles/led-parallel-series-calculator}{amplifiedparts.com: LED Parallel/Series Calculator}). 

We used an 880 nm bandpass filter in front of the Raspberry Pi NoIR camera module to selectively pass the IR LED light. This filter is essential to block the intense microscope laser light, which will obscure the image of the fish by saturating (and likely damaging) the camera sensor. Notably, this filter it is the most expensive part in the setup, costing more than the computer and camera, combined (\autoref{tab:hardware}). With our typical 2-photon GFP/GCaMP imaging settings and the laser tuned to 930nm, laser light is not visible in the camera image. Using such a bandpass filter in the 880 nm range should allow this system to be compatible with many other imaging modalities (confocal, epifluorescence, brightfield, etc), provided that the excitation wavelengths are not in the $\approx$870-900nm range, and the microscope system effectively filters out the 890nm light from the LEDs. If necessary, these wavelength characteristics can be adapted using different LED and filter components. 

To house the system components I used a 3D printed mount (\autoref{fig:2}B,C). This was designed using FreeCAD  (\href{https://www.freecad.org/}{freecad.org}, \href{https://github.com/owenrandlett/pi_tailtrack/blob/main/3d_printing/pi_tailtrack_mount.FCStd}{FreeCAD file}), and 3D printed in black PETG using and Creality Ender 3 Pro 3D printer. It consists of the main body shape that holds the the camera, IR filter, red stimulus LEDs above the fish, and IR LEDs in the oblique illumination configuration (\href{https://github.com/owenrandlett/pi_tailtrack/blob/main/3d_printing/pi_tailtrack_mount_MainShape.stl}{Main Shape}). An insert is placed into the depression above the IR filter, forming the platform onto which the zebrafish dish is placed (\href{https://github.com/owenrandlett/pi_tailtrack/blob/main/3d_printing/pi_tailtrack_mount_CameraDepressionInsert.stl}{Depression Insert}). The final 3D printed component is a semicircular shape that completes the encirclement of the objective, and helps minimize light scattering (\autoref{fig:2}Bi, \href{https://github.com/owenrandlett/pi_tailtrack/blob/main/3d_printing/pi_tailtrack_mount_Semicircle.stl}{Semicircle STL file}).  

I would note that I built up the size of the platform of the mount to match with the relatively spacious configuration of the microscope I was using (\autoref{fig:2}C). A much more compact configuration is possible, since we only require $\approx$26 mm of clearance from the fish to the bottom of the $\approx$6 mm thick camera. The base design could be easily adapted to match different microscope stage configurations. For example the entire system could be inverted to accommodate an inverted microscope (though performing behavioural experiments on inverted zebrafish may be sub-optimal). Or, if stimuli need to be bottom-projected, a small 45-degree hot mirror could be used to divert the image to the camera and free the space directly beneath the animal for stimuli. 

\subsection{Software and tail tracking strategy}

Tracking objects in images and videos has undergone a revolution with deep learning and neural network frameworks, where the tracking and reconstruction of complex animal postures is possible after training networks on only a few example images \citep{Mathis2018-rw, Pereira2022-rd}. However, such approaches are computationally intensive and generally require dedicated and GPU hardware beyond the capabilities of the standard Raspberry Pi, making them incompatible with our project goals. In contexts where the image background is predictable and stable, classical computer vision methods like background subtraction, filtering and thresholding may still be preferable to network-based object identification, especially when speed or computational resources are priorities. Here I have used the \emph{Numpy} \citep{harris2020array} and \emph{OpenCV} \citep{opencv_library} libraries to handle the image data and computer vision tasks (\autoref{fig:1}). 

\begin{figure}
\begin{fullwidth}
\begin{center}

\includegraphics[width=0.65\linewidth]{Figure_TrackingResults.png}
\caption{\textbf{Larval zebrafish trail tracking examples.}
\\ \textbf{A)} Screenshot of a single frame of a tracking image, showing the image from the camera ("Camera Image") with the resultant tracking points overlayed as white dots. The final tracking point is shown as a white semicircle, which is used in the coordinate search algorithm. "Threshold" shows the result of the Adaptive Thresholding operation, and "Threshold + Filtering" the result of the morphological Opening and Closing operations. Displayed along the top are the: frame (current frame number of the experiment), sec\_elapsed (number of seconds elapsed in the experiment), fps (current frame rate, frames per second), stim\_val (the current value read on the stimulus recording pin: GPIO Pin 4). A schematic of the image field, depicting the agarose mounting medium, the position of the zebrafish, and the microscope objective visible in the background is shown in the left panel.
\\ \textbf{B)} i) Example frames during a swimming event. ii) Tail angle deflections during 3 distinct swim events. iii) 3D plot of tail coordinates through the same time period as (ii), drawn in the same time color code. 
\\ \textbf{C)} Same as (B), but for a period in which the larvae executes a struggle/escape maneuver and associated high amplitude tail deflections. 
}
\label{fig:3}

\videosupp{Screen recording of the tail tracking example, \href{https://github.com/owenrandlett/pi_tailtrack/raw/main/SupplementaryVideo_tailtracking_20211213.mkv}{download}}\label{video:S1}


\end{center}
\end{fullwidth}
\end{figure}

Image frames are acquired directly from the camera buffer as an 8-bit Numpy array, and thresholded using Adaptive Thresholding (\emph{cv2.adaptiveThreshold}) to identify bright objects in the image (\autoref{fig:3}, "Threshold"). This binary image is then filtered using a morphological Opening and Closing operation (\emph{cv2.morphologyEx}). This combination generally results in a nicely segmented fish blob in the final binary image (\autoref{fig:3}A, "Threshold + Filtering"). However, this method identifies all large bright objects in the image, including borders of the agarose block and reflections on the microscope objective, and therefore we need a method to identify the fish object among these various segmented blobs.

The fish object is identified with a pre-defined coordinate that acts as the first tracking point of the fish. The fish object is then skeletonized into up to 10 tail segements (\autoref{fig:3}A, 'Tracking Pts'), which can be used to reconstruct the posture of the tail to identify swimming events (\autoref{fig:3}B,C). To do this skeletonization, the tracking points are iteratively identified based on the intersection of a semicicle and the fish object, offset 7 pixels from the previous tracking point, and oriented in the direction of the previous segment (similar to \cite{Stih2019-gx, Randlett2019-fj}). For the first search, this direction is toward the right of the image. Therefore, this strategy relies on the zebrafish larvae being oriented with its tail pointed towards the right, and being placed in the same position such that the exit point of the tail from the agarose block intersects with the fist tracking point. It also requires that no other bright objects intersect with the fish object after binarization. Therefore, it is critical to avoid distracting objects in the imaging scene, such as scratches in the dish or stray pieces of agarose.  

This computationally lean segmentation and skeletonization strategy takes less than 10 ms on the Raspberry Pi CPU. The imaging frame rate when using the \emph{picamera} python package will adjust based on the throughput of the analysis system, which can change with the complexity of the binary images that are processed or external CPU demands, but runs at approximately 104hz (\autoref{fig:3}, 'fps ='). This is sufficient to clearly distinguish different types of movement events, such as swims from escapes/struggles \autoref{fig:3}B vs C), and where individual tail beats during swimming events are resolvable. However, this will not be true during rapid/burst swimming, in which tail-beat frequency will be exceed our frame rate. If such temporal resolution is required our setup will be insufficient, and we will will only reliably track tail half-beat frequencies of $\leq$50hz. Therefore, this system is not capable of comprehensive behavioural characterization, but is impressively capable considering this is done with 26€ camera and a 95€ computer. 

During the experiment the software provides a visual display, is as shown in the screenshots in (\autoref{fig:3}), and screen capture video (\autoref{fig:3}--\autoref{video:S1}). Results of the thresholding, filtering, and skeleton tracking are visible and updated in real-time. This can be used to optimize the position of the zebrafish, the Adaptive Thresholding parameters (neighborhood, threshold) using the \emph{'w/a/s/d'} keys, and the position of the first tracking point using the arrow keys.


\subsection{Output data format}

The tail tracking data are saved in a comma-separated text file \emph{'*\_coords.txt'}, the 10 pairs of "X" and "Y" coordinates for each tail point are saved as rows, and thus there are two rows with 10 columns for every tracked frame. 'NaN' values represent instances where a tail point is not identified. 

The timing of the data is saved in a separate text file \emph{'*\_tstamps.txt'}, which also has two rows for each frame. The first value is the "timestamp" reflecting the time elapsed since the beginning of the tracking experiment. This is used to relate the tail posture and behavioural events to specific points in time. This is important for experiments in which precise timing of behavioural events is important, because the frame rate is not fixed and can fluctuate during the experiment (see above).

The second value in the \emph{'*\_tstamps.txt'} file is the value recorded on one of the GPIO pin 4 of the Raspberry Pi. This value will read either "low"=0 for a voltage less than 1.8V, or "high"=1 for 1.8-3.3V. I use these recordings to synchronize the behavioural recordings with the frames recorded on the microscope. In our typical setup we are using an analog output pin from the DAQ board on the microscope to control the red stimulis LEDs (\autoref{fig:2}A), and we also connect this output of the DAQ board to GPIO pin 4 on the microscope. In this way, we can synchronize the stimuli, microscope imaging frames, and the behavioural recordings. 

These datasets can be read into python for analysis using, for example: 

\begin{adjustwidth}{2cm}{}
\begin{lstlisting}[language=Python]
import numpy as np

# load tracking coordinates
data = np.loadtxt('*_coords.txt', delimiter=',') 

# separate 'x' and 'y' coordinates of tracking points
x_coords = data[::2, :] 
y_coords = data[2::2, :]

# load timing data
t_stamps = np.loadtxt('*_tstamps.txt', delimiter=',')

# separate 'timestamps' and 'stimulus state' recordings
time = t_stamps[::2]
stim = t_stamps[1::2]

\end{lstlisting}
\end{adjustwidth}

\subsection{Behavioural analysis of Ca\textsuperscript{2+} imaging data}

\begin{figure}
\begin{fullwidth}
\begin{center}

\includegraphics[width=1\linewidth]{Figure_FunctionalAnalysis.png}
\caption{\textbf{Identification of behaviour-associated neurons in a larval zebrafish brain via 2-photon Ca\textsuperscript{2+} imaging.}
\\ \textbf{A)} Histogram for the mean tail angle during individual movement bouts for a single larvae over an 80 minute imaging session. Bouts are classified as left or right turns based on a threshold value of 0.004 radians/bout. 
\\ \textbf{B)} Histogram for the bout vigor, quantified using a rolling standard deviation of absolute tail angle. Movements are classified as "swims" or "struggles" based on a threshold value of 0.017 (AU: arbitrary units).
\\ \textbf{C)} Tuning of Ca\textsuperscript{2+} traces in ROIs to turns to the left (green) or right (magenta), as classified in (A). Images are the Pearson correlation coefficient to each behavioral regressor (left or right turns), scaled from 0.0 to 0.3. \emph{Tg2(elavl3:GCaMP6s)} expression pattern is shown in grey. Arrows highlight the Anterior Rhombencaphalic Turning Region (ARTR): with isilateral tuning to turning direction. A = Anterior, P = Posterior
\\ \textbf{D, E)} Tuning of neurons to swims \textbf{(D)}, and struggles \textbf{(E)}, as classified in (B).  
}
\label{fig:4}

\end{center}
\end{fullwidth}
\end{figure}

To test the performance of the \emph{pi\_tailtrack} system, I analyzed Ca\textsuperscript{2+} imaging data from an 80 minute-long volumetric recording covering a large proportion of the brain (as in \cite{Lamire2023-di}). To identify neurons tuned to behavioural parameters I used "regressors" derived from the \emph{pi\_tailtrack} recordings reflecting different motor states convolved with the GCaMP response kernel (as in \cite{Miri2011-sr}). Zebrafish swim bouts can be classified as either forward swims or turns, and an area within the anterior hindbrain is associated with turning direction. This area is known as the Anterior Rhombencephalic Turning Region (ARTR: \cite{Dunn2016-bg}, also called the HBO: \cite{Ahrens2013-xh, Wolf2017-ma}), and shows a conspicuous activity pattern with stripes of neurons tuned to the ipsilateral turning direction. By looking at correlations to regressors reflecting right and left turns, I identified these stripes of neurons in the ARTR-region, indicating that I can successfully identify the ARTR using \emph{pi\_tailtrack} (\autoref{fig:4}A,C). A similar analysis looking at "swims" vs "struggles", with "struggles" reflecting high-amplitude tail flicking events (\autoref{fig:3}C, \autoref{fig:4}B), identified differential neuronal activation in the context of these two movement categories (\autoref{fig:4}B,D,E), with the presence of lateral hindbrain populations of neurons that were negatively correlated with "swims", and a broader and more positively correlated population with "struggles". 

\subsection{Future developments}

Here I have used the \emph{pi\_tailtrack} system to simply record the behaviour of the animal independent of the microscopy or any stimulus delivery. Therefore, the timing of microscope image acquisition is controlled by the microscope computer and is independent of \emph{pi\_tailtrack}. These separate experimental clocks (microscope frames vs Pi Camera frames) must be synchronized, and in our case I have used the GPIO input pin on the Raspberry Pi to record the timing of the stimuli delivered by the microscope relative to the Pi Camera frames. An alternative solution would be to use the Raspberry Pi to deliver the stimuli, perhaps by integrating a video projector system to allow for the delivery of arbitrary and complex visual stimuli. This would also open up possibilities for performing "virtual reality" experiments, where the behaviour of the animal dictates the stimulus in closed-loop. In some microscope systems it should also be possible to use the Raspberry Pi GPIO to trigger microscope acquisitions. This may be preferable if the synchronization between imaging and behaviour frames is critical. 

It is also important to note that hardware in this micro-computer/Raspberry Pi space is rapidly evolving. Indeed, a new suite of Raspberry Pi V3 Cameras have just been released, offering increased resolution, dynamic range, and frame rate. Using these cameras, we may be able to increase the frame rate of tracking into the multiple-hundreds of hz, which would allow us to more reliably resolve individual tail half-beats. The Raspberry Pi "Global Shutter" Camera has also recently been released, which is likely also going to be very interesting for behavioural neuroscience, as the use of a global shutter avoids rolling shutter artifacts that distort images along the frame during rapid motion. 

\subsection{Conclusion}

Here I described our system for tracking the tail of the larval zebrafish during microscopy. Many of the practical considerations of this setup may be specific to our application, and therefore may need modification for use in other experiments in other labs. However, I feel that the core and simple idea of using an IR-sensitive Raspberry Pi Camera, a simple Python script, coupled with IR LEDs and and IR filter, provides an approachable and flexible solution that may be widely useful for observing and tracking the behaviour of zebrafish (or perhaps other animals) while performing imaging experiments. 

Even if this project is not directly useful to you or your research, I hope it can serve as an example of how by combining rudimentary knowledge of electronics hardware and Python scripting (supplemented with extensive use of Google and StackOverflow), it is possible to construct a very inexpensive but capable system. This system's attributes may also make it an ideal tool for community engagement activities such as school outreach programs. It could serve as a platform for learning about microelectronics, behavioural analyses, machine vision, and hardware design and construction.



\section{Methods}

\subsection{Animals}

All experiments were performed on larval zebrafish at 5 days post fertilization (dpf), raised at a density of $\approx$1 larvae/mL of E3 media in a 14:10h light/dark cycle at 28-29\degree{}C. Adult zebrafish were housed, cared for, and bred at the Lyon PRECI zebrafish facility. \textit{mitfa}/Nacre mutant animals (ZDB-ALT-990423-22) were used to prevent pigmentation. 

Larval zebrafish were mounted and head restrained for 2-photon imaging and behavioural analysis by placing them in a very small drop of E3 in the lid of a 35mm petri dish (Greiner bio-one, 627102). Molten ($\approx$42\degree{}C) 2\% low melting point agarose (Sigma A9414) in E3 Medium was added to the dish in an approximately 10mm-diameter droplet around the fish, and the zebrafish was repositioned within the solidifying agarose using a gel-loading pipette tip, such that it was oriented symmetrically for imaging with the dorsal surface of the head at the surface of the agarose. After the agarose had solidified ($\approx$10 minutes), E3 was added to the dish, and then the agarose around the tail was cut away. This was done using an scalpel in two strokes emanating laterally from just below the swim bladder (illustrated in \autoref{fig:3}A). It is critical to not scratch the dish in the vicinity of the freed tail, which can interfere with tail-tracking. 

\subsection{Animal Ethics Statement}

Adult zebrafish used to generate larvae were housed in accordance with PRCI facility approved by the animal welfare committee (comité d’éthique en expérimentation animale de la Région Rhône-Alpes: CECCAPP, Agreement \# C693870602). Behaviour and microscopy experiments were performed at the 5dpf stage, and are thus not subject to ethical review, but these procedures do not harm the larvae. 

\subsection{Software}

Software was written in Python, using the \emph{picamera} library for camera control \citep{picamera}. Trail tracking was performed using \emph{OpenCV} (cv2 version 4.5.5) \citep{opencv_library}, and \emph{Numpy} (version 1.19.5) \citep{harris2020array}. All code is provided in the file \href{https://github.com/owenrandlett/pi_tailtrack/blob/main/record_tail.py}{record\_tail.py}. The main method for tail extraction was Adaptive Thresholding (cv2.adaptiveThreshold), using a threshold of -10 and a 33 pixel neighborhood. These parameters can be adjusted in real-time using the w/s and a/d keys. The starting coordinate for the tail tracking can be adjusted using the arrow keys. 

Code for generating the figure panels in \autoref{fig:3} can be found in:  \href{https://github.com/owenrandlett/pi_tailtrack/blob/main/plot_tail.ipynb}{plot\_tail.ipynb}. Datasets are available here: \href{https://www.dropbox.com/sh/dbjq2dud1ws1o2v/AACLamthISys8sUD1a5oRcR1a?dl=0}{pi\_tailtrack datasets}.

\subsection{Hardware}

\begin{table}[bt]
\caption{\label{tab:hardware}Bill of Materials}
% Use "S" column identifier to align on decimal point 
\begin{tabular}{l l l r l}

\toprule

Component   & Manufacturer  & Cat. Number  & $\approx$Price (€) & Supplier/Link \\

\midrule

Raspberry Pi Computer & Raspberry Pi Found. 
    & 4B Rev 1.4 8gb 
        & 95 
            & \href{https://www.kubii.com/fr/cartes-raspberry-pi/2955-raspberry-pi-4-modele-b-8gb-3272496309050.html?src=raspberrypi}{kubii} 
\\
Pi NoIR Camera 
    & Raspberry Pi Found. 
        & NoIR v2.1 
            & 26 & \href{https://www.kubii.com/fr/cameras-capteurs/1654-nouvelle-camera-infrarouge-v2-8mp-kubii-5060214370288.html?src=raspberrypi}{kubii} 

\\
24'' Pi Camera Cable 
    & Samtec 
        & FJ-15-D-24.00-4 
            & 18 
                & \href{https://fr.farnell.com/en-FR/samtec/fj-15-d-24-00-4/cable-assy-15p-same-sided-610mm/dp/3514891}{farnell} 

\\
880nm IR Bandpass filter
    & Edmund Optics 
        & 65-122
            & 177
                & \href{https://www.edmundoptics.com/p/880nm-cwl-125mm-dia-hard-coated-od-4-10nm-bandpass-filter/19776/}{Edmund Optics} 
\\
890nm LEDs
    & Vishay Semiconductor
        & TSHF5410
            & 0.35 $\times$ 10 = 4
                & \href{https://fr.rs-online.com/web/p/leds-infrarouges/1652375}{RS Components} 

\\
18V DC power supply\textsuperscript{1}
    & generic, for IR LEDs
        & min $\approx$200mA
            & 15
                & e.g. \href{https://www.amazon.fr/Adaptateur-dalimentation-Chargeur-adapt%C3%A9-1000mA/dp/B09F3CRPZW/ref=sr_1_3?__mk_fr_FR=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2S3D8F1YJAF0A&keywords=18v+power+supply&qid=1683210624&sprefix=18v+power+supply%2Caps%2C77&sr=8-3}{amazon.fr}

\\
Current Limiting Resistor\textsuperscript{1}
    & generic, for IR LEDs
        & minimum 1W power
            & 1
                & e.g. \href{https://www.amazon.fr/R%C3%A9sistances-bobin%C3%A9-Anti-surge-Anti-flamme-Watts/dp/B074L3V3MS/ref=asc_df_B074L3V3MS/?tag=googshopfr-21&linkCode=df0&hvadid=227943813093&hvpos=&hvnetw=g&hvrand=241985199445761415&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9056016&hvtargid=pla-403568537008&psc=1}{amazon}

\\
3D printed parts\textsuperscript{2}
    & Black/opaque, generic 
        & PETG\textsuperscript{3}
            & 1
                & \href{https://github.com/owenrandlett/pi_tailtrack/tree/main/3d_printing}{github:pi\_tailtrack}

\\
M3 screws and nuts
    & generic
        & to secure camera
            & 1
                & 

\\
Computer screen, keyboard, mouse
    & generic 
        & recycle/borrow/steal
            & 
                & 
\\
\midrule
    &  
        & 
            & \textbf{\underline{Total: $\approx$338€}}
                & 
\\

\bottomrule
\end{tabular}

\medskip 
\textsuperscript{1}These particular specs are not required, but the power supply and resistor must be matched appropriately. See \href{https://www.amplifiedparts.com/tech-articles/led-parallel-series-calculator}{amplifiedparts.com: LED Parallel/Series Calculator}
\\
\textsuperscript{2}Parts were printed on an \href{https://www.creality.com/products/ender-3-pro-3d-printer}{Ender 3 Pro} 3D printer: Price $\approx$200€.
\\
\textsuperscript{3}This is the material I had on hand, but likely anything will work (PLA, ABS, Resin, etc)

\end{table}

I used a Raspberry Pi 4 Model B Rev 1.4 computer, running Raspbian GNU/Linux 11 (bullseye). \autoref{tab:hardware} contains the details of the hardware components that I used, their approximate price, and an option for supplier (keeping in mind that these later two are subject to change and will rapidly become inaccurate). 

\subsection{Ca\textsuperscript{2+} imaging and analysis}

2-photon Ca2+ imaging was performed and analyzed as described in \citep{Lamire2023-di}. Briefly, a 5dpf \emph{Tg2(elavl3:GCaMP6s)} (ZDB-ALT-180502-2, \cite{Dunn2016-bg}) larvae was imaged using a 20x 1.0NA water dipping objective (Olympus) on a Bruker Ultima microscope at the CIQLE imaging platform (Lyon, LYMIC). Frames were acquired using a resonant scanner over a rectangular region of 1024×512 pixels (0.6\micro m x/y resolution) and piezo objective to scan 12 planes separated at 10\micro m steps, with a repeat rate of 1.98 hz. ROIs were identified and fluorescence timeseries extracted using suite2p \citep{Pachitariu2016-tl}. The zebrafish was stimulated with 60 "dark flash" stimuli at 60 second ISI \citep{Lamire2023-di}, though responses to these stimuli were not incorporated into the analyses presented here, other than to synchronize the behavioural tracking with the microscope acquisition timing.

Code for generating the figure panels in \autoref{fig:4} can be found in: \href{https://github.com/owenrandlett/pi_tailtrack/blob/main/gcamp_corr_swimming.ipynb}{gcamp\_corr\_swimming.ipynb}. Datasets are available here: \href{https://www.dropbox.com/sh/dbjq2dud1ws1o2v/AACLamthISys8sUD1a5oRcR1a?dl=0}{pi\_tailtrack datasets}. Images output from the analysis were adjusted for brightness/contrast and LUT using FIJI/ImageJ \citep{Schindelin2012-hd}.

\section{Acknowledgements}

This work was supported by funding from the ATIP-Avenir program of the CNRS and Inserm, a Fondation Fyssen research grant, and the IDEX-Impulsion initiative of the University of Lyon.

\bibliography{References_pi_tailtrack}

\end{document}
supplement